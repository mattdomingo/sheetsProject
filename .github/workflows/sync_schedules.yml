name: Sync Cloud Scheduler Jobs based on Template

on:
  push:
    branches:
      - main
    paths:
      - 'table_insert_queries/**.sql'       # Trigger on changes to SQL files
      - 'cloud_scheduler_configs/schedule_template.yaml' # Trigger on changes to the template

permissions:
  contents: read # To checkout the code
  # id-token: write # Required for Workload Identity Federation (if used)

jobs:
  sync-cloud-scheduler:
    runs-on: ubuntu-latest
    # Add permissions needed to interact with Cloud Scheduler
    # These might vary depending on how you configure the SA
    # permissions:
    #   # Example if using WIF
    #   id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Needed for git diff

      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v2
        with:
          # Option 1: Workload Identity Federation (Recommended)
          # workload_identity_provider: 'projects/<YOUR_PROJECT_NUMBER>/locations/global/workloadIdentityPools/<YOUR_POOL_ID>/providers/<YOUR_PROVIDER_ID>'
          # service_account: 'your-scheduler-admin-sa@your-gcp-project-id.iam.gserviceaccount.com' # Needs roles/cloudscheduler.admin
          # Option 2: Service Account Key JSON (Using your existing secret)
          credentials_json: '${{ secrets.DEPLOYMENT_SA }}' # Ensure this SA has roles/cloudscheduler.admin or equivalent

      - name: Set up gcloud CLI
        uses: google-github-actions/setup-gcloud@v2

      - name: Install yq (YAML processor)
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq
          sudo chmod +x /usr/bin/yq
        shell: bash

      - name: Install jq (JSON processor)
        run: sudo apt-get update && sudo apt-get install -y jq
        shell: bash

      - name: Sync Scheduler Jobs
        run: |
          TEMPLATE_FILE="cloud_scheduler_configs/schedule_template.yaml"
          SQL_DIR="table_insert_queries"
          GCS_BUCKET="testmain-tenant-dev-sql-storage" # Bucket where SQL files are copied
          JOB_PREFIX="sql-refresh-" # Prefix for Cloud Scheduler job names
          LOCATION_ID="us-central1" # CHANGE THIS if your scheduler jobs are in a different region
          PROJECT_ID=anthem-snacks

          echo "--- Reading template settings from $TEMPLATE_FILE ---"
          if [[ ! -f "$TEMPLATE_FILE" ]]; then
            echo "ERROR: Template file '$TEMPLATE_FILE' not found. Cannot sync jobs." >&2
            exit 1
          fi

          # Read settings from template using yq
          schedule=$(yq '.schedule' "$TEMPLATE_FILE")
          target_url=$(yq '.target_url' "$TEMPLATE_FILE")
          time_zone=$(yq '.time_zone' "$TEMPLATE_FILE")
          desc_template=$(yq '.description_template' "$TEMPLATE_FILE")
          http_method=$(yq '.http_method' "$TEMPLATE_FILE")
          attempt_deadline=$(yq '.attempt_deadline' "$TEMPLATE_FILE")

          # Basic validation
          if [[ -z "$schedule" || -z "$target_url" || -z "$time_zone" || -z "$desc_template" || -z "$http_method" ]]; then
             echo "ERROR: Template file '$TEMPLATE_FILE' is missing one or more required fields." >&2
             exit 1
          fi

          echo "Template Schedule: $schedule"
          echo "Template Target URL: $target_url"
          echo "Template Time Zone: $time_zone"
          echo "Template HTTP Method: $http_method"

          # --- Upsert jobs based on current SQL files ---
          echo "\n--- Syncing jobs for SQL files in $SQL_DIR/ ---"
          # Use find to handle filenames with spaces, etc., correctly
          # Store current SQL files in an associative array for easy lookup later
          declare -A current_sql_files
          find "$SQL_DIR" -maxdepth 1 -name '*.sql' -print0 | while IFS= read -r -d $'\0' sql_file_path; do
            sql_filename=$(basename "$sql_file_path")
            current_sql_files["$sql_filename"]=1 # Mark file as present

            echo "Processing SQL file: $sql_filename"

            # Construct GCS path
            sql_gcs_path="gs://${GCS_BUCKET}/${SQL_DIR}/${sql_filename}"

            # Derive job name (replace .sql, replace underscores/dots with hyphens)
            job_name_suffix=$(echo "$sql_filename" | sed 's/\.sql$//g' | tr '._' '--')
            job_name="${JOB_PREFIX}${job_name_suffix}"

            # Create description from template
            description=$(echo "$desc_template" | sed "s/{SQL_FILENAME}/$sql_filename/g")

            # Construct the message body (JSON containing the GCS path)
            message_body=$(jq -n --arg path "$sql_gcs_path" '{sql_gcs_path: $path}')

            echo "Ensuring job '$job_name' exists and is up-to-date..."

            gcloud scheduler jobs update http "$job_name" \
              --project="$PROJECT_ID" \
              --location="$LOCATION_ID" \
              --schedule="$schedule" \
              --time-zone="$time_zone" \
              --uri="$target_url" \
              --http-method="$http_method" \
              --message-body="$message_body" \
              --description="$description" \
              ${attempt_deadline:+--attempt-deadline="$attempt_deadline"} # Add deadline only if set in template

            if [[ $? -ne 0 ]]; then
              echo "ERROR: Failed to create/update job $job_name for SQL file $sql_filename. Failing workflow." >&2
              exit 1
            fi
            echo "Job '$job_name' synced successfully."
            echo ""

          done

          # --- Delete jobs for SQL files that no longer exist ---
          echo "\n--- Deleting jobs for removed SQL files ---"
          # List existing jobs with the specific prefix in the target location
          gcloud scheduler jobs list --project="$PROJECT_ID" --location="$LOCATION_ID" --filter="name ~ ^projects/.*/locations/.*/jobs/${JOB_PREFIX}" --format='value(name)' | while IFS= read -r full_job_name; do
            job_name=$(basename "$full_job_name")
            echo "Checking existing job: $job_name"

            # Try to derive the original SQL filename from the job name
            # Reverse the transformation: remove prefix, replace hyphens with underscore (best guess)
            # This is imperfect if original names had hyphens.
            # A more robust approach would store metadata in the job description/labels if needed.
            possible_sql_suffix=$(echo "$job_name" | sed "s/^${JOB_PREFIX}//" | tr '-' '_')
            possible_sql_filename="${possible_sql_suffix}.sql"

            # Check if the derived SQL filename exists in our current list
            if [[ -v current_sql_files["$possible_sql_filename"] ]]; then
              echo "  Corresponding SQL file '$possible_sql_filename' exists. Job $job_name is valid."
            else
              echo "  Corresponding SQL file '$possible_sql_filename' NOT FOUND in $SQL_DIR/. Deleting job $job_name..."
              gcloud scheduler jobs delete "$job_name" --project="$PROJECT_ID" --location="$LOCATION_ID" --quiet
              if [[ $? -eq 0 ]]; then
                 echo "  Successfully deleted job $job_name."
              else
                 # Don't fail the workflow, but log clearly
                 echo "  WARNING: Failed to delete job $job_name. Manual cleanup might be needed." >&2
              fi
            fi
          done

          echo "\nCloud Scheduler sync completed."
        shell: bash 