name: Execute Changed BQ Insert Queries and Copy to GCS

on:
  push:
    branches:
      - main # Trigger on push/merge to main branch
    paths:
      # Only trigger if relevant SQL files change
      - 'table_insert_queries/**.sql'

permissions:
  contents: read # Required to checkout the code
  # id-token: write # Required for Workload Identity Federation (preferred method)
  # If using SA Key JSON instead of WIF:
  # (No specific permissions needed here, but ensure secret is configured)

jobs:
  execute-and-copy-sql: # Renamed job for clarity
    runs-on: ubuntu-latest
    # Optional: Specify environment if using Workload Identity Federation
    # environment: your-gcp-environment-name

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Fetch entire history to compare changes accurately against the previous commit
          fetch-depth: 0

      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v2
        with:
          # Option 1: Workload Identity Federation (Recommended)
          # workload_identity_provider: 'projects/<YOUR_PROJECT_NUMBER>/locations/global/workloadIdentityPools/<YOUR_POOL_ID>/providers/<YOUR_PROVIDER_ID>'
          # service_account: 'your-bq-gcs-service-account@your-gcp-project-id.iam.gserviceaccount.com' # Needs BQ and GCS permissions
          # Option 2: Service Account Key JSON (Using your secret)
          credentials_json: '${{ secrets.SA_DEPLOYMENT }}' # Ensure this SA has BQ and GCS permissions

      - name: Set up gcloud CLI
        uses: google-github-actions/setup-gcloud@v2
        # No specific version needed, will use latest

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install google-cloud-bigquery

      - name: Identify and execute changed SQL files
        id: execute
        run: |
          echo "Identifying changed SQL files in table_insert_queries/ ..."
          # Compare the current commit (HEAD) with its direct parent (HEAD~1)
          CHANGED_FILES=$(git diff --name-only --diff-filter=ACMRTUXB HEAD~1 HEAD -- 'table_insert_queries/**.sql' || true)

          # Store changed files for potential later use (like copying)
          # Use a delimiter that's unlikely in file names for multi-line output
          # Using EOF marker for multi-line output
          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          if [[ -z "$CHANGED_FILES" ]]; then
            echo "No changed *.sql files found in table_insert_queries/. Skipping execution and copy."
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0 # Exit successfully
          fi

          echo "Found changed files to execute:"
          echo "$CHANGED_FILES"
          echo "-------------------------------------"

          # Prepare arguments for Python script
          echo "$CHANGED_FILES" | xargs -r python - << EOF
          import sys
          import os
          import logging
          from google.cloud import bigquery
          from google.cloud.exceptions import GoogleCloudError

          logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

          def execute_sql(client: bigquery.Client, sql_content: str, source_file: str):
              logging.info(f"Executing SQL from {source_file}...")
              statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip() and not stmt.strip().startswith('--')]
              job = None
              try:
                  for statement in statements:
                      if not statement: continue
                      logging.info(f"Running: {statement[:150]}...") # Log more context
                      job = client.query(statement)
                      job.result() # Wait for completion
                      logging.info(f"Successfully executed statement from {source_file}.")
                  return True
              except GoogleCloudError as e:
                  logging.error(f"BigQuery error executing SQL from {source_file}: {e}")
                  if job and job.errors:
                       for error in job.errors:
                          logging.error(f"  BQ Error Detail: {error['message']}")
                  return False
              except Exception as e:
                  logging.error(f"Unexpected error executing SQL from {source_file}: {e}")
                  return False

          if __name__ == "__main__":
              sql_files_to_execute = sys.argv[1:]
              if not sql_files_to_execute:
                  logging.warning("No SQL files provided to execute script.")
                  sys.exit(0) # Exit successfully if no files passed

              # Client uses Application Default Credentials setup by google-github-actions/auth
              bq_client = bigquery.Client()
              logging.info(f"Using BigQuery Client with project: {bq_client.project}")

              all_successful = True
              for file_path in sql_files_to_execute:
                  logging.info(f"--- Processing file: {file_path} ---")
                  try:
                      with open(file_path, 'r') as f:
                          content = f.read()
                      if not execute_sql(bq_client, content, file_path):
                          all_successful = False
                  except FileNotFoundError:
                      logging.error(f"File not found: {file_path}")
                      all_successful = False
                  except Exception as e:
                       logging.error(f"Failed to read file {file_path}: {e}")
                       all_successful = False

              if not all_successful:
                  logging.error("One or more SQL files failed execution.")
                  sys.exit(1) # Fail the step

              logging.info("All changed SQL files executed successfully.")
              sys.exit(0) # Succeed the step
          EOF
          # --- [ END OF PYTHON SCRIPT ] ---

          # If the python script exited with non-zero status, this step will fail
          echo "status=executed" >> $GITHUB_OUTPUT

      - name: Copy changed files to GCS
        # Run only if the previous step succeeded and actually executed files
        if: success() && steps.execute.outputs.status == 'executed'
        run: |
          GCS_BUCKET="testmain-tenant-dev-sql-storage" # Target bucket
          echo "Copying processed files to gs://${GCS_BUCKET}/ ..."
          echo "Files to copy:"
          echo "${{ steps.execute.outputs.changed_files }}"
          echo "-------------------------------------"

          # Loop through the files captured in the output
          # Use process substitution to handle multi-line output correctly
          while IFS= read -r file_path; do
            if [[ -n "$file_path" ]]; then # Ensure the line is not empty
              echo "Copying $file_path to gs://${GCS_BUCKET}/$file_path"
              gsutil cp "$file_path" "gs://${GCS_BUCKET}/$file_path"
            fi
          done <<< "${{ steps.execute.outputs.changed_files }}"

          echo "GCS copy step completed."

      # Optional: Use this output in subsequent steps if needed
      # - name: Check Execution Status
      #   run: echo "SQL Execution status was ${{ steps.execute.outputs.status }}" 