name: Rebuild Changed BQ Tables and Copy to GCS

on:
  push:
    branches:
      - main # Trigger on push/merge to main branch
    paths:
      # Only trigger if relevant SQL files change
      - 'table_create_queries/**.sql'
      - 'table_insert_queries/**.sql' # Also trigger on insert file changes/deletions

permissions:
  contents: read # Required to checkout the code
  # id-token: write # Required for Workload Identity Federation (preferred method)
  # If using SA Key JSON instead of WIF:
  # (No specific permissions needed here, but ensure secret is configured)

jobs:
  rebuild-and-copy-sql:
    runs-on: ubuntu-latest
    # Optional: Specify environment if using Workload Identity Federation
    # environment: your-gcp-environment-name

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Fetch entire history to compare changes accurately against the previous commit
          fetch-depth: 0

      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v2
        with:
          # Option 1: Workload Identity Federation (Recommended)
          # workload_identity_provider: 'projects/<YOUR_PROJECT_NUMBER>/locations/global/workloadIdentityPools/<YOUR_POOL_ID>/providers/<YOUR_PROVIDER_ID>'
          # service_account: 'your-bq-gcs-service-account@your-gcp-project-id.iam.gserviceaccount.com' # Needs BQ and GCS permissions
          # Option 2: Service Account Key JSON (Using your secret)
          credentials_json: '${{ secrets.DEPLOYMENT_SA }}' # Ensure this SA has BQ and GCS permissions

      - name: Set up gcloud CLI
        uses: google-github-actions/setup-gcloud@v2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install google-cloud-bigquery

      - name: Handle deleted files in GCS
        # Always run this step after auth, even if later steps fail
        run: |
          GCS_BUCKET="testmain-tenant-dev-sql-storage" # Target bucket (same as copy step)
          echo "Identifying deleted SQL files..."
          # Look for deleted files in both directories
          DELETED_FILES=$(git diff --name-only --diff-filter=D HEAD~1 HEAD -- 'table_create_queries/**.sql' 'table_insert_queries/**.sql' || true)

          if [[ -z "$DELETED_FILES" ]]; then
            echo "No deleted *.sql files found in monitored directories. Skipping GCS removal."
            exit 0
          fi

          echo "Found deleted files to remove from GCS:"
          echo "$DELETED_FILES"
          echo "-------------------------------------\"

          while IFS= read -r file_path; do
            if [[ -n "$file_path" ]]; then # Ensure the line is not empty
              gcs_path="gs://${GCS_BUCKET}/${file_path}"
              echo "Attempting to remove $gcs_path from GCS..."
              # Use gsutil rm. Adding -f ensures it doesn't fail if the object is already gone.
              if gsutil -q rm "$gcs_path"; then
                echo "Successfully removed $gcs_path"
              else
                # Check if the file simply didn't exist, which is okay.
                # gsutil stat returns non-zero for non-existent files.
                if ! gsutil -q stat "$gcs_path"; then
                   echo "Object $gcs_path did not exist in GCS. No action needed."
                else
                   echo "WARNING: Failed to remove $gcs_path. It might still exist."
                   # Decide if this should be a hard failure (exit 1) or just a warning.
                   # For now, just warn, as the primary goal is BQ execution.
                fi
              fi
            fi
          done <<< "$DELETED_FILES"

          echo "GCS deletion step completed."

      - name: Identify and execute changed SQL pairs (Create then Insert)
        id: execute
        run: |
          echo "Identifying changed CREATE SQL files..."
          CHANGED_CREATE_FILES=$(git diff --name-only --diff-filter=ACMRTUXB HEAD~1 HEAD -- 'table_create_queries/**.sql' || true)
          echo "Identifying changed INSERT SQL files..."
          CHANGED_INSERT_FILES=$(git diff --name-only --diff-filter=ACMRTUXB HEAD~1 HEAD -- 'table_insert_queries/**.sql' || true)

          if [[ -z "$CHANGED_CREATE_FILES" && -z "$CHANGED_INSERT_FILES" ]]; then
            echo "No changed *.sql files found in monitored directories. Skipping execution."
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Changed CREATE files:"
          echo "${CHANGED_CREATE_FILES:-<none>}"
          echo "Changed INSERT files:"
          echo "${CHANGED_INSERT_FILES:-<none>}"
          echo "-------------------------------------"

          # Define the inline Python executor script
          PYTHON_EXEC_SCRIPT=$(cat << 'EOF'
          import sys, os, logging
          from google.cloud import bigquery
          from google.cloud.exceptions import GoogleCloudError
          logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
          def execute_sql(client, sql_content, source_file):
              logging.info(f"Executing SQL from {source_file}...")
              statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip() and not stmt.strip().startswith('--')]
              job = None
              try:
                  for statement in statements:
                      if not statement: continue
                      logging.info(f"Running: {statement[:150]}...")
                      job = client.query(statement)
                      job.result() # Wait for completion
                      logging.info(f"Successfully executed statement from {source_file}.")
                  return True
              except GoogleCloudError as e:
                  logging.error(f"BigQuery error executing SQL from {source_file}: {e}")
                  if job and job.errors:
                       for error in job.errors: logging.error(f"  BQ Error Detail: {error['message']}")
                  return False
              except Exception as e:
                  logging.error(f"Unexpected error executing SQL from {source_file}: {e}")
                  return False

          if __name__ == "__main__":
              sql_files_to_execute = sys.argv[1:]
              if not sql_files_to_execute:
                  logging.warning("No SQL files provided to execute script.")
                  sys.exit(0)
              # Assuming auth setup provides ADC
              bq_client = bigquery.Client()
              logging.info(f"Using BigQuery Client with project: {bq_client.project}")
              all_successful = True
              for file_path in sql_files_to_execute:
                  logging.info(f"--- Processing file: {file_path} ---")
                  try:
                      # Check if file exists before trying to open
                      if not os.path.exists(file_path):
                          logging.error(f"File not found: {file_path}")
                          all_successful = False
                          continue # Skip to next file
                      with open(file_path, 'r') as f: content = f.read()
                      if not execute_sql(bq_client, content, file_path): all_successful = False
                  except Exception as e:
                      logging.error(f"Failed to read or process file {file_path}: {e}")
                      all_successful = False
              if not all_successful: sys.exit(1) # Fail the step
              logging.info("Python script finished processing provided SQL files.")
              sys.exit(0) # Succeed the step
          EOF
          )

          OVERALL_SUCCESS=true
          declare -A PROCESSED_INSERTS # Associative array to track processed inserts
          FILES_TO_COPY="" # Store paths of ALL files successfully processed for GCS

          # 1. Process CREATE file changes (and their corresponding INSERTs)
          if [[ -n "$CHANGED_CREATE_FILES" ]]; then
            echo "--- Processing CREATE changes ---"
            while IFS= read -r create_file; do
              if [[ -z "$create_file" ]]; then continue; fi
              echo "" # Add spacing
              echo "--- Processing pair starting with CREATE file: $create_file ---"

              table_name=$(basename "$create_file" .sql)
              insert_file="table_insert_queries/${table_name}.sql"

              CREATE_SUCCESS=false
              INSERT_SUCCESS=false # Assume insert might not run or might fail

              # Execute CREATE file
              echo "Executing CREATE file: $create_file"
              if echo "$create_file" | xargs -r python -c "$PYTHON_EXEC_SCRIPT"; then
                echo "Successfully executed $create_file."
                CREATE_SUCCESS=true
                FILES_TO_COPY+="$create_file"$'\n' # Add create file to copy list on success
              else
                echo "ERROR: Failed to execute $create_file."
                OVERALL_SUCCESS=false
                # Continue to next create file, do not attempt insert for this pair
                continue
              fi

              # If CREATE succeeded, execute corresponding INSERT file if it exists
              if [[ "$CREATE_SUCCESS" == "true" ]]; then
                if [[ -f "$insert_file" ]]; then
                  echo "Executing corresponding INSERT file: $insert_file"
                  if echo "$insert_file" | xargs -r python -c "$PYTHON_EXEC_SCRIPT"; then
                    echo "Successfully executed $insert_file."
                    INSERT_SUCCESS=true
                    FILES_TO_COPY+="$insert_file"$'\n' # Add insert file to copy list on success
                    PROCESSED_INSERTS["$insert_file"]=1 # Mark insert as processed
                  else
                    echo "ERROR: Failed to execute $insert_file after successful CREATE. Rebuild incomplete."
                    OVERALL_SUCCESS=false
                    # Continue to next create file, pair failed
                    continue
                  fi
                else
                  echo "INFO: No corresponding insert file '$insert_file' found for '$create_file'. Only CREATE was executed."
                  # This is not necessarily an error, maybe the table has no initial data.
                fi
              fi

            done <<< "$CHANGED_CREATE_FILES"
          fi

          # 2. Process remaining INSERT file changes (those not processed as part of a pair)
          if [[ -n "$CHANGED_INSERT_FILES" ]]; then
            echo "--- Processing standalone/remaining INSERT changes ---"
            while IFS= read -r insert_file; do
              if [[ -z "$insert_file" ]]; then continue; fi

              # Check if this insert was already processed as part of a CREATE pair
              if [[ -v PROCESSED_INSERTS["$insert_file"] ]]; then
                echo "Skipping already processed INSERT file: $insert_file"
                continue
              fi

              echo "" # Add spacing
              echo "--- Processing standalone INSERT file: $insert_file ---"

              # Execute INSERT file
              if echo "$insert_file" | xargs -r python -c "$PYTHON_EXEC_SCRIPT"; then
                 echo "Successfully executed $insert_file."
                 FILES_TO_COPY+="$insert_file"$'\n' # Add to copy list on success
                 PROCESSED_INSERTS["$insert_file"]=1 # Mark as processed (though likely redundant here)
              else
                 echo "ERROR: Failed to execute standalone $insert_file."
                 OVERALL_SUCCESS=false
                 # Continue processing other insert files even if one fails
              fi

            done <<< "$CHANGED_INSERT_FILES"
          fi


          # Check overall success and output files for copying
          if [[ "$OVERALL_SUCCESS" == "true" ]]; then
            echo "-------------------------------------"
            echo "All relevant SQL files executed successfully."
            echo "processed_files_for_copy<<EOF" >> $GITHUB_OUTPUT
            echo -n "$FILES_TO_COPY" >> $GITHUB_OUTPUT # Use -n to avoid extra newline
            echo "EOF" >> $GITHUB_OUTPUT
            echo "status=executed" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "-------------------------------------"
            echo "One or more SQL execution steps failed."
            echo "processed_files_for_copy<<EOF" >> $GITHUB_OUTPUT # Still output successfully processed files
            echo -n "$FILES_TO_COPY" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1 # Fail the job
          fi

      - name: Copy processed files to GCS
        # Run always to copy any files that succeeded, even if the overall job failed
        # But only run if the execute step wasn't skipped entirely
        if: steps.execute.outputs.status != 'skipped'
        run: |
          GCS_BUCKET="testmain-tenant-dev-sql-storage" # Target bucket
          echo "Copying successfully processed files (even if some steps failed) to gs://${GCS_BUCKET}/ ..."
          # Ensure the output exists before trying to read it
          if [[ -z "${{ steps.execute.outputs.processed_files_for_copy }}" ]]; then
            echo "No files were successfully processed or marked for copying."
            exit 0
          fi
          echo "Files to copy:"
          echo "${{ steps.execute.outputs.processed_files_for_copy }}"
          echo "-------------------------------------"
          # Use process substitution to loop through multi-line output
          while IFS= read -r file_path; do
            if [[ -n "$file_path" ]]; then # Ensure the line is not empty
              echo "Copying $file_path to gs://${GCS_BUCKET}/$file_path"
              gsutil cp "$file_path" "gs://${GCS_BUCKET}/$file_path"
            fi
          done <<< "${{ steps.execute.outputs.processed_files_for_copy }}"

          echo "GCS copy step completed." 